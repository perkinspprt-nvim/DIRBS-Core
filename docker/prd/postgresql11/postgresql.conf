#
# Sample postgresql.conf for Docker-ized PostgreSQL deployment.
#
#
# Copyright (c) 2018-2021 Qualcomm Technologies, Inc.
#
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the
# limitations in the disclaimer below) provided that the following conditions are met:
#
# - Redistributions of source code must retain the above copyright notice, this list of conditions and the following
#  disclaimer.
# - Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following
#  disclaimer in the documentation and/or other materials provided with the distribution.
# - Neither the name of Qualcomm Technologies, Inc. nor the names of its contributors may be used to endorse or promote
#  products derived from this software without specific prior written permission.
# - The origin of this software must not be misrepresented; you must not claim that you wrote the original software.
#  If you use this software in a product, an acknowledgment is required by displaying the trademark/logo as per the
#  details provided here: https://www.qualcomm.com/documents/dirbs-logo-and-brand-guidelines
# - Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software.
# - This notice may not be removed or altered from any source distribution.
#
# NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY
# THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
# BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

# Set config file locations
data_directory = '/data/db'
hba_file = '/data/config/pg_hba.conf'
ident_file = '/data/config/pg_ident.conf'

# Listen on external interfaces
listen_addresses = '*'

# Make sure UNIX sockets get created in correct directory
unix_socket_directories = '/run/postgresql'

# Make sure we use SSL
ssl = true
ssl_cert_file = '/etc/ssl/certs/ssl-cert-snakeoil.pem'
ssl_key_file = '/etc/ssl/private/ssl-cert-snakeoil.key'

# Connectivity
max_connections = 100  # VARIES: Change to match number of expected concurrent conns
max_prepared_transactions = 110  # VARIES: Should be more than max_connections
superuser_reserved_connections = 3
max_locks_per_transaction  = 256

# Loading libraries and preventing spinlock contentions
shared_preload_libraries = 'pg_stat_statements'
huge_pages = off

# Memory Settings
shared_buffers = '128 GB'  # VARIES: Set to one quarter of RAM
work_mem = '512 MB'  # VARIES: About 1MB per 1GB or RAM in machine
maintenance_work_mem = '2720 MB'  # VARIES: About 5x work_mem
temp_buffers = '512 MB'  # VARIES: Set to work_mem

effective_cache_size = '358 GB'  # VARIES: Roughtly total RAM size - shared_buffers
effective_io_concurrency = 4  # VARIES: Roughly 0.5 num spindles for HDD or 100+ for SSD

# Background writer
bgwriter_delay = 200ms
bgwriter_lru_maxpages = 100
bgwriter_lru_multiplier = 2.0
bgwriter_flush_after = 0

# Replication
wal_level = replica
max_wal_senders = 10
synchronous_commit = on
wal_keep_segments = 4100

# WAL writing
wal_compression = on
wal_buffers = -1
wal_writer_delay = 200ms
wal_writer_flush_after = 1MB

# Checkpointing:
checkpoint_timeout = '15 min'
max_wal_size = '32768 MB'  # VARIES: Depends on size of DB
min_wal_size = '16384 MB'  # VARIES: Depends on size of DB
checkpoint_completion_target = 0.9

# Tuned statistics target - default of 100 is too low for large table
default_statistics_target = 1000

# Parallel queries:
max_worker_processes = 28  # VARIES: Set to number of CPU cores
max_parallel_workers_per_gather = 14  # VARIES: Set to 0.5 * max_worker_processes
max_parallel_workers = 28  # VARIES: Set to number of CPU cores

# Logging settings
log_line_prefix = '%t:%r:%u@%d:[%p]: '
log_checkpoints = on
log_temp_files = 0
log_autovacuum_min_duration = 250
log_lock_waits = on

# Stats tuning - put temp directory on ramdisk for stats
stats_temp_directory = '/run/postgresql'

